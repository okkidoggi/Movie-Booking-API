{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/okkidoggi/Movie-Booking-API/blob/master/rag_pdf.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qr5qPt0TWqs0"
      },
      "source": [
        "# Creating a PDF Question and Answer System Using Retrieval-Augmented Generation\n",
        "\n",
        "---\n",
        "\n",
        "### Introduction\n",
        "\n",
        "In this tutorial, we will create a Question and Answer (Q&A) system that uses Retrieval-Augmented Generation (RAG) to answer questions about the contents of a PDF file. We will be using Langchain and OpenAI to build this system, which will enable us to extract information intelligently and efficiently.\n",
        "\n",
        "This guide is designed to be straightforward, breaking down the process into simple, easy-to-follow steps. Whether you're new to coding or have some experience, you will find everything you need to get started on your own intelligent Q&A system.\n",
        "\n",
        "We will be using ChatGPT as our Language Model (LLM) to add a conversational aspect to our Q&A system.\n",
        "\n",
        "### Steps to Create Your Q&A System\n",
        "\n",
        "**Step 1: Install Required Libraries**  \n",
        "To get started, we need to install all the libraries necessary for our project. Open your command line or terminal and run the following command:\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install langchain_community langchain_chroma langchain_openai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mCoWohE1XUNQ",
        "outputId": "c8615dcf-3cd4-4a70-b700-e96c4717bfda"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/50.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.2 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.2/1.2 MB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m19.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hi3exk8OWqs3",
        "outputId": "1e43db21-04c2-474d-947b-3eadb1e5c270"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/67.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m615.7/615.7 kB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m53.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m298.0/298.0 kB\u001b[0m \u001b[31m22.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m273.8/273.8 kB\u001b[0m \u001b[31m19.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.7/94.7 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m53.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.2/93.2 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.3/13.3 MB\u001b[0m \u001b[31m85.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.5/52.5 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.7/149.7 kB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.0/64.0 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.4/54.4 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.7/63.7 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m442.1/442.1 kB\u001b[0m \u001b[31m33.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.3/73.3 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m79.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m425.7/425.7 kB\u001b[0m \u001b[31m31.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m164.1/164.1 kB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip -q install langchain openai chromadb pypdf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z1GWAjPRWqs4"
      },
      "source": [
        "**Step 2: Initialize Embeddings and the Language Model**  \n",
        "Now, we need to set up the embeddings and load the ChatGPT model. This code snippet will help you do just that:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "aduZ4iu4Wqs4"
      },
      "outputs": [],
      "source": [
        "# Load required libraries\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "from langchain.chains.question_answering import load_qa_chain\n",
        "\n",
        "from langchain_chroma import Chroma\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "from langchain_openai.llms import OpenAI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "clr3n0dcWqs5",
        "outputId": "5ca99cec-6140-4262-e71a-80b41a6e7ece"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-9-22911d5eeb11>:7: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import ChatOpenAI``.\n",
            "  llm = ChatOpenAI(model_name=\"gpt-4o-mini\")\n"
          ]
        }
      ],
      "source": [
        "# Set your OpenAI API key\n",
        "load_dotenv()\n",
        "OpenAI.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "\n",
        "# Load the embedding and LLM model\n",
        "embeddings_model = OpenAIEmbeddings()\n",
        "llm = ChatOpenAI(model_name=\"gpt-4o-mini\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d_GvZSYkWqs5"
      },
      "source": [
        "**Step 3: Perform Q&A Over a PDF File**  \n",
        "Next, we wil set up our Q&A system to work with a PDF file. You will need to provide either the link to where your PDF is hosted or the local path on your computer where the PDF is stored.\n",
        "\n",
        "For our example, we will use a research paper in PDF format for our Q&A tasks.\n",
        "Simply download the PDF, place it in your current working directory, and provide its path in the following variable:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "l3J_xeAgWqs5"
      },
      "outputs": [],
      "source": [
        "pdf_link = \"attention_paper_3295222.3295349.pdf\"\n",
        "loader = PyPDFLoader(pdf_link, extract_images=False)\n",
        "pages = loader.load_and_split()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pdf_link2 = \"prompt_engineering.pdf\"\n",
        "loader2 = PyPDFLoader(pdf_link2, extract_images=False)\n",
        "pages2 = loader2.load_and_split()"
      ],
      "metadata": {
        "id": "WNawiNuTas2t"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DYWuiFpCWqs5"
      },
      "source": [
        "Once we have successfully extracted the data from the PDF, we’ll break it into smaller, more manageable chunks using the `RecursiveCharacterTextSplitter` from Langchain. This is essential because it helps us deal with the token limitations of the LLM models, allowing us to process the data more effectively."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "Iyd3bZMYWqs6"
      },
      "outputs": [],
      "source": [
        "# Split data into chunks\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=4000,\n",
        "    chunk_overlap=20,\n",
        "    length_function=len,\n",
        "    add_start_index=True,\n",
        ")\n",
        "chunks = text_splitter.split_documents(pages)\n",
        "chunks2 = text_splitter.split_documents(pages2)\n",
        "# Combine chunks from both PDFs\n",
        "all_chunks = chunks + chunks2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MFYyo02iWqs6"
      },
      "source": [
        "**Step 4: Create Embeddings and Store Them in the Vector Database**  \n",
        "Now, we’re ready to create embeddings for the chunks we just split and store them in a vector database. We’ll be using Chroma as our vector database. Here’s how to do that:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "0pbMLhk9Wqs6"
      },
      "outputs": [],
      "source": [
        "# Store data into the database\n",
        "db = Chroma.from_documents(all_chunks, embedding=embeddings_model, persist_directory=\"test_index\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s5zucWuvWqs6"
      },
      "source": [
        "In this step, you will provide the chunk data you want to create an embedding for, specify the model used for creating the embedding, and set the directory where the database will be stored for future use.\n",
        "\n",
        "**Step 5: Load the Existing Database**  \n",
        "Once the information is safely stored in the database, you don’t have to repeat the previous steps every time. Instead, we can load the pre-existing database using the following code snippet:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KoCTYnfUWqs7",
        "outputId": "499017d6-844e-49bd-8070-1284021f1501"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-13-bf3ae087425e>:6: LangChainDeprecationWarning: This class is deprecated. See the following migration guides for replacements based on `chain_type`:\n",
            "stuff: https://python.langchain.com/docs/versions/migrating_chains/stuff_docs_chain\n",
            "map_reduce: https://python.langchain.com/docs/versions/migrating_chains/map_reduce_chain\n",
            "refine: https://python.langchain.com/docs/versions/migrating_chains/refine_chain\n",
            "map_rerank: https://python.langchain.com/docs/versions/migrating_chains/map_rerank_docs_chain\n",
            "\n",
            "See also guides on retrieval and question-answering here: https://python.langchain.com/docs/how_to/#qa-with-rag\n",
            "  chain = load_qa_chain(llm, chain_type=\"stuff\")\n"
          ]
        }
      ],
      "source": [
        "# Load the database\n",
        "vectordb = Chroma(persist_directory=\"test_index\", embedding_function=embeddings_model)\n",
        "\n",
        "# Load the retriever\n",
        "retriever = vectordb.as_retriever(search_kwargs={\"k\": 3})\n",
        "chain = load_qa_chain(llm, chain_type=\"stuff\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EeLLFO1pWqs7"
      },
      "source": [
        "The retriever will be responsible for fetching the most relevant chunk from the database that might contain the answer to the user’s question. In this example, the `search_kwargs` parameter, with `k` set to 3, ensures we retrieve the top three most relevant chunks from the database.\n",
        "\n",
        "**Step 6: Create a Function to Generate Responses**  \n",
        "Next, we will create a function that helps generate answers to user questions. This function will take the user’s question as input, retrieve the relevant information, and use the Q&A chain to provide a response."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "hHcJ1Wa4Wqs7"
      },
      "outputs": [],
      "source": [
        "# A utility function for answer generation\n",
        "def ask(question):\n",
        "    context = retriever.invoke(question)\n",
        "    answer = chain.invoke({\"input_documents\": context, \"question\": question})\n",
        "    return answer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VMVZG3peWqs7"
      },
      "source": [
        "**Step 7: Ask Questions and Get Answers**  \n",
        "Now, we are ready to use our Q&A system! To ask a question, simply use the following lines of code in your script:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BjZsYj0AWqs7",
        "outputId": "7391390b-2ae3-433b-f4e6-d5f1cc4b565a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "User: why is the sky blue?\n",
            "Answer: I don't know.\n"
          ]
        }
      ],
      "source": [
        "# Take the user input and call the function to generate output\n",
        "user_question = input(\"User: \")\n",
        "\n",
        "answer = ask(user_question)\n",
        "print(\"Answer:\", answer['output_text'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BnjYMzViWqs8"
      },
      "source": [
        "In this tutorial, we built a RAG Q&A system using Langchain and OpenAI, demonstrating how to combine advanced language models with effective data processing. We covered all the key steps, from installing libraries to performing Q&A on PDF data.\n",
        "\n",
        "This guide empowers you to enhance your projects with dynamic Q&A features. By using Langchain and OpenAI, you can transform simple questions into meaningful conversations, paving the way for more interactive applications. As you start your own projects, keep in mind the potential of merging language models with data processing."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "ace",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}